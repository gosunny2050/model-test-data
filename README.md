# model-test-data

1. AI2D（Abstractive Illustrated Diagram Dataset）
    核心目标：评估模型对科学图表（如生物循环图、物理原理图）的抽象理解能力。

    数据特点：

    包含 5,000+ 教育类图表（主要来自小学教材）。

    每张图表对应多个问答对，需结合图表结构和文本标签推理。

    任务示例：
    “根据光合作用示意图，指出氧气产生的阶段对应的结构名称。”

    挑战点：理解层级关系、符号含义及跨模态逻辑关联。

    适用场景：教育类 AI 助手、教材内容解析工具。

2. MathVista
    核心目标：测试视觉化数学问题的解决能力（融合几何、统计、代数等）。

    数据特点：

    6,141 个问题，含函数图像、几何图形、数据可视化图表。

    需结合数学公式与视觉元素进行推理。

    任务示例：
    “根据折线图计算某商品在第二季度的平均增长率。”

    挑战点：数学符号与视觉元素的跨模态对齐。

    评估指标：准确率（需分步推理过程可解释）。

3. MMBench（Multimodal Benchmark）
    核心目标：评估通用多模态模型的综合能力（感知+推理）。

    数据特点：

    3,000+ 样本，覆盖图像分类、问答、文本推理等任务。

    包含中英文双语测试集。

    任务示例：
    “根据菜单图片和用户对话历史，推荐适合的菜品。”

    v1.1 升级：
    ✅ 新增动态时序推理（如视频关键帧分析）
    ✅ 增强对抗性干扰（遮挡、噪声等）。

4. MME（Multimodal Multilingual Evaluation）
    核心目标：多语言多任务统一评估框架。

    数据特点：

    24 个子任务，涵盖 OCR、情感分析、常识推理等。

    支持 10+ 语言（含中文、日文、阿拉伯文等）。

    任务示例：
    “分析西班牙语广告图中的文字和图像，判断目标受众群体。”

    优势：单次测试即可获得模型多维度能力雷达图。

5. MMMU（Massive Multi-discipline Multimodal Understanding）
    核心目标：测试跨学科（大学级别）多模态理解能力。

    数据特点：

    11.5K 高质量问答，涵盖艺术、工程、医学等 6 大学科。

    需解析论文插图、实验数据图等专业内容。

    任务示例：
    “根据电磁学实验波形图，解释谐振频率变化原因。”

    挑战点：学科专业知识与多模态推理的结合。

6. HallusionBench
    核心目标：检测模型对视觉幻觉/逻辑矛盾的识别能力。

    数据特点：

    包含 500+ 视觉陷阱图像（如不可能物体、镜像文字）。

    每张图对应真/假命题判断问题。

    任务示例：
    “图中镜子里的时钟是否显示真实时间？”

    评估重点：模型是否被光学错觉误导。

7. ScienceQA
    核心目标：评估科学领域（STEM）的多模态问答能力。

    数据特点：

    21,208 个问题，结合图表、公式和文本。

    覆盖物理、化学、生物等学科。

    任务示例：
    “根据电路图判断当开关闭合时哪个灯泡会亮。”

    附加数据：包含学生答题时的眼动追踪数据（用于可解释性研究）。

8. OCRBench
    核心目标：测试复杂场景下的文字识别鲁棒性。

    数据特点：

    包含弯曲文本、低光照、艺术字体等挑战性样本。

    需同时识别文字并理解语义（如路牌指示）。

    任务示例：
    “识别古建筑匾额上的篆书文字并翻译为现代汉语。”

    评估指标：字符级准确率 + 语义连贯性评分。

9. TextVQA
    核心目标：验证图像内文本理解与推理能力。

    数据特点：

    28,408 个问题，需结合图中文字回答。

    场景涵盖街景、产品包装、文档等。

    任务示例：
    “根据公交车侧面的广告文字，回答推广的旅游景点名称。”

    关键技术：OCR + 语义理解的端到端整合。

10. RealWorldQA
    核心目标：评估对现实场景的认知与常识推理能力。

    数据特点：

    收集自真实环境照片（超市、街道、家庭等）。

    问题涉及安全判断、物体功能理解等。

    任务示例：
    “根据厨房照片，指出砧板摆放位置是否合理。”

    应用场景：服务机器人环境理解、自动驾驶场景认知。

11. SeedBench
    核心目标：为多模态大模型提供细粒度评估标准。

    数据特点：

    19K 样本，覆盖 23 种视觉-语言任务类型。

    强调细粒度属性识别（如材质、纹理、空间关系）。

    任务示例：
    “比较两件家具图片，指出哪一个是柚木材质。”

    创新点：引入“多选问答”形式减少模型偏差。

12. MMYet
    核心目标：针对东亚文化场景的多模态理解测试。

    数据特点：

    包含汉字书法、传统服饰、节日习俗等文化元素。

    问题需结合文化背景知识解答。

    任务示例：
    “根据茶道仪式图片，判断正在进行的是哪个步骤。”

    适用模型：需本地化文化适应的多模态AI。

    总结对比
    数据集	核心能力侧重	数据规模	典型应用领域
    MMMU	跨学科专业知识	11.5K	教育、学术研究
    TextVQA	图文结合推理	28K+	广告分析、文档处理
    HallusionBench	抗幻觉能力	500+	安全关键系统（如医疗）
    RealWorldQA	现实场景常识	10K+	服务机器人、自动驾驶
    如需某个数据集的官方论文或下载链接，可进一步说明！ 📊


    3B多模态视频理解模型
    https://huggingface.co/AI-Safeguard/Ivy-VL-llava